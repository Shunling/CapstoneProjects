{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project 2: GPCR research trend\n",
    "## Natural Language Processing of a domain specific literature\n",
    "***\n",
    "### Word2Vec vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import warnings \n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import itertools as it\n",
    "warnings.filterwarnings(action = 'ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class callback(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.loss_s = 0\n",
    "        \n",
    "    def on_epoch_end(self,model):\n",
    "            \n",
    "        if self.epoch % 5 == 0:\n",
    "            loss_e = int(model.get_latest_training_loss())\n",
    "            loss = loss_e - self.loss_s\n",
    "            self.loss_s = loss_e\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
    "        self.epoch += 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = LineSentence('./phrases/fourgrams.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['allosteric', 'agonist', 'of', '7tm', 'receptor', 'expand', 'the', 'pharmacological_toolbox', 'approximately', '1', 'of', 'the', 'genome', 'of', 'high', 'organism', 'encode', 'seven_transmembrane_7tm_g_protein_couple', 'receptor', 'which', 'control', 'an', 'extensive', 'range', 'of', 'physiological_process', 'and', 'represent', 'drug_target', 'for', 'nearly_half', 'of', 'all', 'drug', 'that', 'be', 'prescribe', 'currently', 'to', 'date', 'most', 'drug', 'that', 'target', '7tm', 'receptor', 'interact', 'via', 'the', 'same', 'domain', 'as', 'the', 'endogenous', 'agonist', 'call', 'the', 'orthosteric_site', 'however', 'the', 'advent', 'of', 'functional', 'screening_assay', 'have', 'greatly_increase', 'the', 'number', 'of', 'allosteric_ligand', 'identify', 'such', 'ligand', 'bind', 'to', 'topographically_distinct_site', 'on', '7tm', 'receptor', 'in', 'addition', 'to', 'modulate', 'the', 'affinity', 'of', 'orthosteric_ligand', 'allosteric_ligand', 'can', 'also', 'alter', 'the', 'efficacy', 'of', 'orthosteric_ligand', 'and', 'activate', '7tm', 'receptor', 'in', 'their_own_right', 'in', 'this_article_we', 'briefly_review', 'the', 'current_status', 'of', 'putative', 'allosteric', 'agonist', 'of', '7tm', 'receptor', 'and', 'discuss', 'the', 'promise', 'and', 'challenge', 'that', 'this', 'class', 'of', 'ligand', 'may', 'pose', 'for', 'pharmacologist', 'and', 'the', 'drug_discovery', 'industry']\n"
     ]
    }
   ],
   "source": [
    "for line in it.islice(text,1,2):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 78321928\n",
      "Loss after epoch 5: 55895800\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_size = 300 # word vector dimensionality\n",
    "window_context = 50 # context window size\n",
    "min_word_count = 5 # minimum word count\n",
    "sample = 1e-5 # Downsample setting for frequent words\n",
    "model1 = Word2Vec(text, #fourgrams abstract list tokenized in words\n",
    "                 size = feature_size,\n",
    "                 window = window_context,\n",
    "                 min_count = min_word_count,\n",
    "                 sample = sample,\n",
    "                 sg=1,\n",
    "                 compute_loss = True,\n",
    "                 iter = 7,\n",
    "                 workers = 4,\n",
    "                 callbacks=[callback()]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final Loss after epoch 7\n",
    "model1.get_latest_training_loss() - 78321928 - 55895800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save('./models/m300_50_5_5_iter7.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
