{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project 2: GPCR research trend\n",
    "## Natural Language Processing of a domain specific literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import itertools as it\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shunling/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 323646 entries, 0 to 323645\n",
      "Data columns (total 10 columns):\n",
      "Unnamed: 0     323646 non-null int64\n",
      "Id             323646 non-null int64\n",
      "abstract       296557 non-null object\n",
      "title          323646 non-null object\n",
      "authors        323646 non-null object\n",
      "journal        323646 non-null object\n",
      "journal_abv    323646 non-null object\n",
      "affiliation    323646 non-null object\n",
      "keywords       37845 non-null object\n",
      "year           323546 non-null float64\n",
      "dtypes: float64(1), int64(2), object(7)\n",
      "memory usage: 24.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./Clean_data/clean_pub.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>journal_abv</th>\n",
       "      <th>affiliation</th>\n",
       "      <th>keywords</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24877594</td>\n",
       "      <td>Abstract    It has been well appreciated tha...</td>\n",
       "      <td>What we know and do not know about the cannab...</td>\n",
       "      <td>['Malfitano AM', 'Basu S', 'Maresz K', 'Bifulc...</td>\n",
       "      <td>Seminars in immunology.</td>\n",
       "      <td>Semin Immunol</td>\n",
       "      <td>\\n Dipartimento di Medicina e Chirurgia, Unive...</td>\n",
       "      <td>KEYWORDS:   Cannabinoid receptor 2; Endocann...</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16889837</td>\n",
       "      <td>Abstract    Approximately 1% of the genome o...</td>\n",
       "      <td>Allosteric agonists of 7TM receptors: expandi...</td>\n",
       "      <td>['Langmead CJ', 'Christopoulos A']</td>\n",
       "      <td>Trends in pharmacological sciences.</td>\n",
       "      <td>Trends Pharmacol Sci</td>\n",
       "      <td>\\n Psychiatry Centre of Excellence for Drug Di...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        Id                                           abstract  \\\n",
       "0           0  24877594    Abstract    It has been well appreciated tha...   \n",
       "1           1  16889837    Abstract    Approximately 1% of the genome o...   \n",
       "\n",
       "                                               title  \\\n",
       "0   What we know and do not know about the cannab...   \n",
       "1   Allosteric agonists of 7TM receptors: expandi...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  ['Malfitano AM', 'Basu S', 'Maresz K', 'Bifulc...   \n",
       "1                 ['Langmead CJ', 'Christopoulos A']   \n",
       "\n",
       "                               journal           journal_abv  \\\n",
       "0              Seminars in immunology.         Semin Immunol   \n",
       "1  Trends in pharmacological sciences.  Trends Pharmacol Sci   \n",
       "\n",
       "                                         affiliation  \\\n",
       "0  \\n Dipartimento di Medicina e Chirurgia, Unive...   \n",
       "1  \\n Psychiatry Centre of Excellence for Drug Di...   \n",
       "\n",
       "                                            keywords    year  \n",
       "0    KEYWORDS:   Cannabinoid receptor 2; Endocann...  2014.0  \n",
       "1                                                NaN  2006.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text Preprocessing:</h2>\n",
    "<br/>\n",
    "    <li> Combine abstarct and title to text\n",
    "    <li>  Normalize text:\n",
    "    <ol>\n",
    "        <ol>\n",
    "        <li> convert to lower case\n",
    "        <li> remove excessive space\n",
    "        <li> remove puncta\n",
    "        <li> lemmatize text   \n",
    "        <li> phrase modeling\n",
    "           </ol>\n",
    "        </ol>\n",
    "    <li> Clean keywords\n",
    "    <li> Extract affiliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_text(text):\n",
    "    try:\n",
    "        text = text.lower()\n",
    "    except:\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "def reduce_space(text):\n",
    "    \"\"\"\n",
    "    remove eccessive space and leave only one\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = re.sub(r' +', ' ',text)\n",
    "    except:\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "def remove_puncta(text):\n",
    "    \"\"\" \n",
    "    only retain word, greek word, numbers\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pattern = r'[^a-z0-9α-ѕ\\s\\-]'\n",
    "        text = re.sub(pattern, '', text)\n",
    "    except:\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', parse=True, tag=True, entity=True)\n",
    "def lemmatize_text(text): \n",
    "    \"\"\" \n",
    "    normalize word variant forms to the same word\n",
    "    no NaN data would be\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = nlp(text)\n",
    "        text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    except:\n",
    "        pass\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe to store the processed data\n",
    "df_new = df[['Id','year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shunling/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# combine abstract and title to text\n",
    "df['abstract'] = df['abstract'].fillna('')\n",
    "df_new['text'] = df['title'] + df['abstract']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 59min 27s, sys: 4min 8s, total: 3h 3min 36s\n",
      "Wall time: 3h 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# reduce space\n",
    "# lower case\n",
    "# remove puncta\n",
    "# lemmatize_text\n",
    "\n",
    "def replace_abstract(text):\n",
    "    try:\n",
    "        text = text.replace(' abstract ', '')\n",
    "    except:\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "text = df_new.text.apply(lower_text)\n",
    "text = text.apply(replace_abstract)\n",
    "text = text.apply(reduce_space)\n",
    "text = text.apply(remove_puncta)\n",
    "text = text.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shunling/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_new['text'] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 53s, sys: 2.11 s, total: 5min 55s\n",
      "Wall time: 5min 57s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shunling/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# lower case\n",
    "# delete the start with keywords \n",
    "# reduce space\n",
    "# lemmatize_text\n",
    "def replace_keyword(text):\n",
    "    try:\n",
    "        text = text.replace('keywords:', '')\n",
    "    except:\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "text = df.keywords.apply(lower_text)\n",
    "text = text.apply(replace_keyword)\n",
    "text = text.apply(reduce_space)\n",
    "text = text.apply(lemmatize_text)\n",
    "df_new['keywords'] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract affiliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "top22_pharma = ['johnson','roche','pfizer','novartis','merk','lilly','novo nordisk', 'abbvie', 'amgen','sanofi',\\\n",
    "               'glaxosmithkline','astrazeneca','gilead','squibb','csl','takeda','bayer','allergan','kgaa',\\\n",
    "                'boehringer','biogen','abbott']\n",
    "\n",
    "def get_company(text):\n",
    "    \"\"\" assign affilliation info if affiliation is one of the top22 pharmaceutical companies \"\"\"\n",
    "    \n",
    "    for company in top22_pharma:\n",
    "        if company in text:\n",
    "            return company\n",
    "\n",
    "def get_university(text):\n",
    "    \"\"\" assign affiliation to university\"\"\"\n",
    "    pattern = r',(.*? university)'\n",
    "    \n",
    "    university = re.findall(pattern, text)\n",
    "\n",
    "    return university   \n",
    "\n",
    "    \n",
    "def extract_affiliation(text):\n",
    "    \"\"\"\n",
    "    simplify affiliation to:\n",
    "        top20 pharmaceutical companies\n",
    "        university (distinguishable in english)\n",
    "        others\n",
    "    \"\"\"\n",
    "\n",
    "    # if it's top22 pharma\n",
    "    affiliation = get_company(text)\n",
    "    \n",
    "    # if not top22 pharma and is a university\n",
    "    if not affiliation:\n",
    "        affiliation = get_university(text)\n",
    "        \n",
    "    return affiliation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.45 s, sys: 39.6 ms, total: 2.49 s\n",
      "Wall time: 2.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = df.affiliation.apply(lower_text)\n",
    "text = text.apply(extract_affiliation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                   []\n",
       "1                                      glaxosmithkline\n",
       "2                      [ the johns hopkins university]\n",
       "3    [ oregon national primate research center, ore...\n",
       "4                             [ washington university]\n",
       "Name: affiliation, dtype: object"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shunling/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_new['affiliation'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv('./Processed_data/year_text_keywords_affiliation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text Preprocessing:</h2>\n",
    "<br>\n",
    "    <li> concat all text to file: corpus\n",
    "    <li> transform to digram\n",
    "    <li> transform to trigram\n",
    "    <li> transform to fourgram\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import itertools as it\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/shunling/Desktop/CapstoneProjects/NaturalLanguageProcessingGPCR/corpus.txt'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currentdir = os.getcwd()\n",
    "def datadir(file):\n",
    "    return currentdir+'/'+file\n",
    "datadir('corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create corpus\n",
    "with open('corpus.txt','w') as f:\n",
    "    for i in range(df_new.text.shape[0]):\n",
    "        f.write(df_new.text.iloc[i])\n",
    "        f.write('.')\n",
    "        f.write('\\n')\n",
    "        \n",
    "# from unigram to digram\n",
    "unigrams = LineSentence(datadir('corpus.txt'))\n",
    "bigram_phrases = Phrases(unigrams) # train model\n",
    "bigram_model = Phraser(bigram_phrases) # build model\n",
    "with open('bigrams.txt','w') as f:\n",
    "    for sent in unigrams:\n",
    "        bigram_sent = ' '.join(bigram_model[sent]) # apply model\n",
    "        f.write(bigram_sent)\n",
    "        f.write('\\n')\n",
    "        \n",
    "# from bigram to trigram\n",
    "bigrams = LineSentence(datadir('bigrams.txt'))\n",
    "trigram_phrases = Phrases(bigrams)\n",
    "trigram_model = Phraser(trigram_phrases)\n",
    "with open('trigrams.txt','w') as f:\n",
    "    for sent in bigrams:\n",
    "        tri_sent = ' '.join(trigram_model[sent])\n",
    "        f.write(tri_sent)\n",
    "        f.write('\\n')\n",
    "        \n",
    "# from trigram to fourgram\n",
    "trigrams = LineSentence(datadir('trigrams.txt'))\n",
    "fourgram_phrases = Phrases(trigrams)\n",
    "fourgram_model = Phraser(fourgram_phrases)\n",
    "with open('fourgrams.txt','w') as f:\n",
    "    for sent in trigrams:\n",
    "        four_sent = ' '.join(fourgram_model[sent])\n",
    "        f.write(four_sent)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ѕ'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'\\u0405'.lower()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
